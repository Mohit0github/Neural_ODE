Model Implementation: Developed a Vision Transformer (ViT) from scratch using PyTorch, incorporating patch embeddings and multi-head self-attention mechanisms.

Dataset and Training: Trained the model on the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits.

Optimization: Employed the Adam optimizer with a learning rate of 0.005 and a CrossEntropyLoss function for training over 5 epochs.

Performance Evaluation: Achieved a test accuracy of approximately 97% on the MNIST dataset after training.
